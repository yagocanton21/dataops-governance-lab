# ğŸš€ DataOps: GovernanÃ§a e Qualidade de Dados

## ğŸ“‹ Sobre o RepositÃ³rio

Este repositÃ³rio contÃ©m o material completo do curso **DataOps, GovernanÃ§a e Qualidade de Dados**, incluindo conceitos teÃ³ricos, laboratÃ³rios prÃ¡ticos com **PySpark** e **Great Expectations**, e um desafio final abrangente.

### ğŸ¯ Objetivos do Curso
- Dominar os **conceitos fundamentais** de DataOps: GovernanÃ§a de Dados
- Implementar **testes de qualidade** automatizados com Great Expectations
- Aplicar as **6 dimensÃµes da qualidade** de dados na prÃ¡tica
- Criar **pipelines DataOps** profissionais e escalÃ¡veis
- Desenvolver **soluÃ§Ãµes completas** de monitoramento e auditoria

## ğŸ“ Estrutura do RepositÃ³rio

```
aulaGovernanÃ§a/
â”œâ”€â”€ ğŸ“š Conceitos.md                           # Fundamentos teÃ³ricos
â”œâ”€â”€ ğŸ¯ Desafio_Final_DataOps.md              # Desafio prÃ¡tico completo
â”œâ”€â”€ ğŸ“Š datasets/                             # Dados para o desafio
â”‚   â”œâ”€â”€ clientes.csv                         # Base de clientes (16 registros)
â”‚   â”œâ”€â”€ produtos.csv                         # CatÃ¡logo de produtos (20 registros)
â”‚   â”œâ”€â”€ vendas.csv                           # TransaÃ§Ãµes de vendas (25 registros)
â”‚   â”œâ”€â”€ logistica.csv                        # Dados de entrega (22 registros)
â”‚   â””â”€â”€ README.md                            # DocumentaÃ§Ã£o dos datasets
â”œâ”€â”€ ğŸ““ notebooks/                            # LaboratÃ³rios prÃ¡ticos
â”‚   â””â”€â”€ Lab_DataOps_Governanca_Qualidade.ipynb  # Lab com Great Expectations
â”œâ”€â”€ ğŸ³ Dockerfile                            # ConfiguraÃ§Ã£o do ambiente
â”œâ”€â”€ ğŸ³ docker-compose.yml                    # OrquestraÃ§Ã£o dos serviÃ§os
â””â”€â”€ ğŸ“– README.md                             # Este arquivo
```

## ğŸ› ï¸ Tecnologias Utilizadas

- **Apache Spark** (PySpark) - Processamento distribuÃ­do de dados
- **Great Expectations** - Framework profissional de validaÃ§Ã£o de dados
- **Jupyter Notebook** - Ambiente interativo de desenvolvimento
- **Docker** - ContainerizaÃ§Ã£o e isolamento do ambiente
- **Pandas** - ManipulaÃ§Ã£o de dados em Python

## ğŸ—ï¸ Arquitetura do Ambiente

### VisÃ£o Geral da Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Jupyter Lab   â”‚    â”‚   Apache Spark   â”‚    â”‚ Great Expect.   â”‚
â”‚   (Port 8888)   â”‚â—„â”€â”€â–ºâ”‚   + Iceberg      â”‚â—„â”€â”€â–ºâ”‚  Data Context   â”‚
â”‚                 â”‚    â”‚   (Port 4040)    â”‚    â”‚   Data Docs     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Docker Network    â”‚
                    â”‚  (172.16.240.0/24)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Data Sources     â”‚
                    â”‚  CSV Files + Spark  â”‚
                    â”‚   Iceberg Tables    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Arquitetura Docker Compose

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Host Machine                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Docker Compose Environment                   â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚           pyspark_aula_container                    â”‚  â”‚  â”‚
â”‚  â”‚  â”‚                                                     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚   Jupyter Lab   â”‚  â”‚  Apache Spark   â”‚          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚   (Port 8888)   â”‚  â”‚  (Port 4040)    â”‚          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚                 â”‚  â”‚  + Iceberg      â”‚          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚                                                     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚Great Expectationsâ”‚  â”‚   Data Warehouseâ”‚          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  Data Context   â”‚  â”‚ /opt/warehouse  â”‚          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚   Data Docs     â”‚  â”‚                 â”‚          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  Network: plataform-network (172.16.240.0/24)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  Volume Mappings:                                               â”‚
â”‚  ./notebooks  â†”  /home/tavares/work                            â”‚
â”‚  ./data       â†”  /home/tavares/data                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes da Arquitetura

#### ğŸ³ **Container Layer**
- **Base Image**: `jupyter/pyspark-notebook`
- **Custom User**: `tavares` (UID: 1001)
- **Working Directory**: `/home/tavares/work`
- **Volumes Mapeados**:
  - `./notebooks` â†’ `/home/tavares/work`
  - `./data` â†’ `/home/tavares/data`

#### ğŸ”¥ **Apache Spark Configuration**
- **VersÃ£o**: Spark 3.3.0 com Hadoop 3
- **Executor Memory**: 4GB
- **Driver Memory**: 4GB
- **Iceberg Support**: Habilitado para Data Lakehouse
- **PostgreSQL Driver**: IncluÃ­do para conectividade

#### ğŸ¯ **Great Expectations Setup**
- **Data Context**: Configurado automaticamente
- **Datasources**: Pandas e Spark
- **Expectation Suites**: Para cada dataset
- **Checkpoints**: AutomaÃ§Ã£o de validaÃ§Ãµes
- **Data Docs**: RelatÃ³rios HTML profissionais

#### ğŸ“Š **Data Architecture**
```
Data Flow:
ğŸ“„ Raw CSV â†’ ğŸ Pandas/Spark â†’ ğŸ¯ Great Expectations â†’ ğŸ“ˆ Data Docs
                    â†“
                ğŸ—„ï¸ Warehouse (Iceberg Tables)
```

### Fluxo de Dados

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw CSV   â”‚â”€â”€â”€â–ºâ”‚   PySpark   â”‚â”€â”€â”€â–ºâ”‚Great Expect.â”‚â”€â”€â”€â–ºâ”‚ Data Docs   â”‚
â”‚   Datasets  â”‚    â”‚  DataFrame  â”‚    â”‚ Validation  â”‚    â”‚  Reports    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Iceberg   â”‚
                   â”‚  Warehouse  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Portas e ServiÃ§os

| ServiÃ§o | Porta | DescriÃ§Ã£o |
|---------|-------|----------|
| **Jupyter Notebook** | 8888 | Interface principal de desenvolvimento |
| **Spark Web UI** | 4040 | Monitoramento de jobs Spark |
| **Data Docs** | File System | RelatÃ³rios Great Expectations |

### Fluxo de Dados

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw CSV   â”‚    â”‚   PySpark   â”‚    â”‚Great Expect.â”‚    â”‚ Data Docs   â”‚
â”‚   Datasets  â”‚    â”‚  DataFrame  â”‚    â”‚ Validation  â”‚    â”‚  Reports    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Iceberg   â”‚
                   â”‚  Warehouse  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PersistÃªncia de Dados

```
Volume Mapping:
ğŸ“ Host                    ğŸ“¦ Container
./notebooks/          â†’   /home/tavares/work/
./data/               â†’   /home/tavares/data/
./datasets/           â†’   AcessÃ­vel via notebooks
```

## ğŸš€ ConfiguraÃ§Ã£o do Ambiente

### OpÃ§Ã£o 1: GitHub Codespaces (Recomendado)

1. **Abra o repositÃ³rio no GitHub Codespaces:**
   ```bash
   # Clique em "Code" > "Codespaces" > "Create codespace on main"
   # Ou use o link direto: https://github.com/seu-usuario/seu-repo/codespaces
   ```

2. **Execute o ambiente com Docker Compose:**
   ```bash
   # No terminal do Codespace
   docker-compose up -d
   ```

3. **Acesse o Jupyter Notebook:**
   - URL: http://localhost:8888
   - Token: `tavares1234`
   - Spark UI: http://localhost:4040

4. **Acesse o laboratÃ³rio:**
   - Abra `notebooks/Lab_DataOps_Governanca_Qualidade.ipynb`
   - Execute as cÃ©lulas sequencialmente

### OpÃ§Ã£o 2: Docker (Ambiente Local)

1. **Clone o repositÃ³rio:**
   ```bash
   git clone https://github.com/seu-usuario/aulaGovernanca.git
   cd aulaGovernanca
   ```

2. **Construa e execute o ambiente:**
   ```bash
   # Construir a imagem Docker
   docker-compose build
   
   # Iniciar os serviÃ§os
   docker-compose up -d
   ```

3. **Acesse o Jupyter Notebook:**
   - URL: http://localhost:8888
   - Token: `tavares1234`
   - Spark UI: http://localhost:4040

4. **Verifique os volumes:**
   ```bash
   # Os notebooks estÃ£o em ./notebooks/
   # Os dados estÃ£o em ./data/
   # Arquivos sÃ£o persistidos automaticamente
   ```

### OpÃ§Ã£o 3: InstalaÃ§Ã£o Local (Python)

1. **PrÃ©-requisitos:**
   ```bash
   # Python 3.8+
   # Java 8 ou 11 (para Spark)
   # Docker e Docker Compose (recomendado)
   ```

2. **Clone e execute com Docker:**
   ```bash
   git clone https://github.com/seu-usuario/aulaGovernanca.git
   cd aulaGovernanca
   docker-compose up -d
   ```

3. **Ou instale manualmente:**
   ```bash
   pip install pyspark==3.3.0 great-expectations pandas jupyter matplotlib
   export JAVA_HOME=/usr/lib/jvm/java-11-openjdk
   jupyter notebook
   ```

## ğŸ“š Como Usar Este RepositÃ³rio

### 1. ğŸ“– Estude os Conceitos
```bash
# Leia primeiro os fundamentos teÃ³ricos
cat Conceitos.md
```

### 2. ğŸ§ª Execute o LaboratÃ³rio
```bash
# Abra o notebook no Jupyter
notebooks/Lab_DataOps_Governanca_Qualidade.ipynb
```

### 3. ğŸ¯ Realize o Desafio
```bash
# Leia as instruÃ§Ãµes do desafio
cat Desafio_Final_DataOps.md

# Use os datasets fornecidos
ls datasets/
```

### 4. ğŸ“Š Explore os Dados
```python
# Exemplo de carregamento dos dados
import pandas as pd

# Carregar datasets
clientes = pd.read_csv('datasets/clientes.csv')
produtos = pd.read_csv('datasets/produtos.csv')
vendas = pd.read_csv('datasets/vendas.csv')
logistica = pd.read_csv('datasets/logistica.csv')

print(f"Clientes: {len(clientes)} registros")
print(f"Produtos: {len(produtos)} registros")
print(f"Vendas: {len(vendas)} registros")
print(f"LogÃ­stica: {len(logistica)} registros")
```

## ğŸ“ Roteiro de Aprendizagem

### MÃ³dulo 1: Fundamentos (30 min)
1. ğŸ“š Leia `Conceitos.md` completamente
2. ğŸ¯ Entenda os 4 pilares da governanÃ§a
3. ğŸ“Š Memorize as 6 dimensÃµes da qualidade

### MÃ³dulo 2: PrÃ¡tica (2 horas)
1. ğŸš€ Configure o ambiente (Docker ou Codespaces)
2. ğŸ§ª Execute o laboratÃ³rio passo a passo
3. ğŸ” Experimente com Great Expectations
4. ğŸ“ˆ Analise os Data Docs gerados

### MÃ³dulo 3: Desafio (1 semana)
1. ğŸ“‹ Leia `Desafio_Final_DataOps.md`
2. ğŸ” Analise os datasets fornecidos
3. ğŸ—ï¸ Implemente a soluÃ§Ã£o completa
4. ğŸ“Š Crie relatÃ³rios profissionais

## ğŸ”§ SoluÃ§Ã£o de Problemas

### Problema: Spark nÃ£o inicia
```bash
# Verifique o Java
java -version

# Configure JAVA_HOME
export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")
```

### Problema: Great Expectations nÃ£o instala
```bash
# Use versÃ£o especÃ­fica
pip install great-expectations==0.18.8

# Ou instale dependÃªncias separadamente
pip install sqlalchemy==1.4.46
pip install great-expectations
```

### Problema: Jupyter nÃ£o acessa
```bash
# Verifique se a porta estÃ¡ livre
netstat -tulpn | grep 8888

# Use porta alternativa
jupyter notebook --port=8889
```

### Problema: Docker nÃ£o funciona
```bash
# Verifique se Docker estÃ¡ rodando
docker --version
docker-compose --version

# Reconstrua a imagem
docker-compose down
docker-compose build --no-cache
docker-compose up
```

## ğŸ“Š Datasets do Desafio

Os datasets simulam uma empresa de e-commerce (**TechCommerce**) com problemas reais de qualidade:

| Dataset | Registros | Problemas Principais |
|---------|-----------|---------------------|
| **clientes.csv** | 16 | Duplicatas, emails invÃ¡lidos, campos vazios |
| **produtos.csv** | 20 | PreÃ§os negativos, categorias vazias, duplicatas |
| **vendas.csv** | 25 | Integridade referencial, datas futuras, cÃ¡lculos incorretos |
| **logistica.csv** | 22 | Datas inconsistentes, campos vazios, duplicatas |

## ğŸ† CritÃ©rios de AvaliaÃ§Ã£o

### LaboratÃ³rio (Formativo)
- âœ… ExecuÃ§Ã£o completa do notebook
- âœ… CompreensÃ£o dos conceitos
- âœ… ExperimentaÃ§Ã£o com Great Expectations

### Desafio (Somativo)
- ğŸ¯ **40%** - ExcelÃªncia tÃ©cnica
- ğŸ“Š **30%** - AplicaÃ§Ã£o dos conceitos
- ğŸ” **20%** - SoluÃ§Ã£o de problemas
- ğŸ“ **10%** - DocumentaÃ§Ã£o

## ğŸ¤ ContribuiÃ§Ã£o

### Como Contribuir
1. Fork o repositÃ³rio
2. Crie uma branch para sua feature
3. FaÃ§a commit das mudanÃ§as
4. Abra um Pull Request

### Reportar Problemas
- Use as **Issues** do GitHub
- Inclua logs de erro completos
- Descreva o ambiente utilizado

## ğŸ“ Suporte

### Canais de Ajuda
- ğŸ’¬ **Discussions**: Para dÃºvidas gerais
- ğŸ› **Issues**: Para reportar bugs
- ğŸ“§ **Email**: professor@exemplo.com

### Recursos Adicionais
- ğŸ“š [DocumentaÃ§Ã£o Great Expectations](https://docs.greatexpectations.io/)
- ğŸ”¥ [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- ğŸ³ [Docker Documentation](https://docs.docker.com/)

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ™ Agradecimentos

- **Apache Spark** pela plataforma de processamento distribuÃ­do
- **Great Expectations** pelo framework de qualidade de dados
- **Jupyter Project** pelo ambiente interativo
- **Docker** pela containerizaÃ§Ã£o

---

## ğŸš€ ComeÃ§e Agora!

### ğŸ³ InÃ­cio RÃ¡pido com Docker (Recomendado)

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/seu-usuario/aulaGovernanca.git
cd aulaGovernanca

# 2. Suba o ambiente
docker-compose up -d

# 3. Acesse o Jupyter
# URL: http://localhost:8888
# Token: tavares1234

# 4. Verifique os serviÃ§os
docker-compose ps
docker logs pyspark_aula_container
```

### ğŸ“‹ PrÃ³ximos Passos
1. **Execute o laboratÃ³rio** - `notebooks/Lab_DataOps_Governanca_Qualidade.ipynb`
2. **Realize o desafio** - `Desafio_Final_DataOps.md`
3. **Torne-se um especialista em DataOps!**

**Que a forÃ§a dos dados esteja com vocÃª!** ğŸ“Šâœ¨

---

*Ãšltima atualizaÃ§Ã£o: $(date +"%Y-%m-%d")*